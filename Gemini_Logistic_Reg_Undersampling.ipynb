{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /Users/tomcio/opt/anaconda3/envs/lobbyingdata_env/lib/python3.11/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/tomcio/opt/anaconda3/envs/lobbyingdata_env/lib/python3.11/site-packages (from imblearn) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/tomcio/opt/anaconda3/envs/lobbyingdata_env/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/tomcio/opt/anaconda3/envs/lobbyingdata_env/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/tomcio/opt/anaconda3/envs/lobbyingdata_env/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/tomcio/opt/anaconda3/envs/lobbyingdata_env/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/tomcio/opt/anaconda3/envs/lobbyingdata_env/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "! pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/tomcio/Documents/GitHub/MIT_MBAn_NER/data/'\n",
    "data = pd.read_csv(path + 'training_data_RAW.csv')\n",
    "X = data[\"Name\"]\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Character-level encoding:\n",
    "\n",
    "Since you want to analyze characters individually, we'll create a character-level vocabulary and convert each word into a vector of character counts. Here, we'll use CountVectorizer for this purpose. Note that we set ngram_range=(1, 1) to consider only single characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 5))\n",
    "X_encoded = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Train-test split:\n",
    "\n",
    "Split the data into training and testing sets for model evaluation.\n",
    "\n",
    "Key Change = Undersampling:  We introduce RandomUnderSampler from imblearn to reduce the majority class. The sampling_strategy=0.5 aims to achieve a 1:1 ratio between the drug name class and the normal word class. Adjust this ratio if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Undersampling (Modify ratio as needed)\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.25)  \n",
    "X_undersampled, y_undersampled = undersample.fit_resample(X_encoded, y)\n",
    "\n",
    "# 4. Split data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_undersampled, y_undersampled, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build a Logistic Regression model:\n",
    "\n",
    "Logistic regression is a good choice for binary classification problems like this. Train the model on the training data.\n",
    "\n",
    "Key Change = Class Weights: During the creation of the LogisticRegression model, we specify class_weight={0: 1, 1: 10} to place more importance on correctly classifying drug names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight={0: 1, 1: 10})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight={0: 1, 1: 10})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight={0: 1, 1: 10})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = LogisticRegression()\n",
    "# model.fit(X_train_res, y_train_res) # changed from X/Y_train to X/Y_train_res\n",
    "\n",
    "# 5. Logistic Regression with class weights\n",
    "model = LogisticRegression(class_weight={0: 1, 1: 10}) \n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate the model:\n",
    "#### Make predictions on the test data and assess the model's performance using metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "Accuracy: The accuracy seems decent (89.9%), implying the model does well on the overall classification task.\n",
    "\n",
    "Precision: A perfect precision of 1.0 indicates that every word the model predicted as a drug name was indeed a drug name (no false positives).\n",
    "\n",
    "Recall: The extremely low recall (0.012) is a significant problem. It suggests that the model is only able to identify a tiny fraction of actual drug names in the dataset. This indicates that the model is too restrictive in its classification.\n",
    "\n",
    "F1-score: The low F1-score (0.024) confirms the poor balance between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8085927770859278\n",
      "Precision: 0.8947368421052632\n",
      "Recall: 0.042579837194740136\n",
      "F1-score: 0.0812910938433951\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Predict for a new word:\n",
    "\n",
    "Once you're satisfied with the model's performance, you can use it to predict the probability of a new word being a drug name. Encode the new word using the same vectorizer and make a prediction using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of metylphenidate being a drug name: 0.4958514267855859\n"
     ]
    }
   ],
   "source": [
    "new_word = \"metylphenidate\"  # Example new word\n",
    "new_word_encoded = vectorizer.transform([new_word])\n",
    "prediction = model.predict_proba(new_word_encoded)[0][1]\n",
    "print(\"Probability of\", new_word, \"being a drug name:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "This approach leverages character-level information instead of relying on whole words or n-grams, considering your requirement.\n",
    "CountVectorizer creates a vocabulary of unique characters and represents each word as a count vector of these characters.\n",
    "Logistic regression classifies each word based on the learned relationship between character features and drug name labels.\n",
    "The final step demonstrates how to use the trained model to predict the probability for a new unseen word.\n",
    "\n",
    "This is a basic example. You can explore more advanced techniques like character-level recurrent neural networks (RNNs) for potentially better performance.\n",
    "Consider handling misspellings or variations in drug names during data preparation or model training.\n",
    "Remember that the model's performance depends on the quality and quantity of your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential problem = Imbalanced Data:\n",
    "\n",
    "It's highly likely that your dataset has far more normal words than drug names. This imbalance forces the model to be conservative to maintain high precision. Here's what you can do:\n",
    "\n",
    "Oversampling: Increase the representation of the drug name class through techniques like SMOTE (Synthetic Minority Oversampling Technique).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
